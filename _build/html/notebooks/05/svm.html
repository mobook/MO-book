
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Support Vector Machines for Binary Classification &#8212; Companion Notebooks for Data-Driven Optimization in Python</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Extra material: Refinery production and shadow pricing with CVXPY" href="refinery-production.html" />
    <link rel="prev" title="Markowitz portfolio optimization" href="markowitz_portfolio.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-DVQ7NZ8CYZ"></script>
<script>
                    window.dataLayer = window.dataLayer || [];
                    function gtag(){ dataLayer.push(arguments); }
                    gtag('js', new Date());
                    gtag('config', 'G-DVQ7NZ8CYZ');
                </script>

  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo-02.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Companion Notebooks for Data-Driven Optimization in Python</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Data-Driven Mathematical Optimization in Python
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../01/01.00.html">
   1. Mathematical Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/production-planning.html">
     A Production Planning Problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/production-planning-basic.html">
     A Basic Pyomo Model
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../01/production-planning-advanced.html">
     A Data-Driven Pyomo Model
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../02/02.00.html">
   2. Linear Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/bim.html">
     BIM production
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/lad-regression.html">
     Least Absolute Deviation (LAD) Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/mad-portfolio-optimization.html">
     MAD portfolio optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/L1-regression-wine-quality.html">
     Wine quality prediction with L1 regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/bim-dual.html">
     Dual of the BIM production problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/bim-maxmin.html">
     BIM production for worst case
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/bim-fractional.html">
     BIM production variants
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/bim-rawmaterialplanning.html">
     BIM production using demand forecasts
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../02/multiproductionfaciliity_worstcase.html">
     Extra material: Multi-product facility production
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../03/03.00.html">
   3. Mixed Integer Linear Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/bim-perturbed.html">
     BIM production with perturbed data
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/shift-scheduling.html">
     Workforce shift scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/simple-production-model-gdp.html">
     Production model using disjunctions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/machine-scheduling.html">
     Machine Scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/recharging-electric-vehicle.html">
     Recharging strategy for an electric vehicle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/bim-production-revisited.html">
     BIM production revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/cryptarithms.html">
     Extra material: Cryptarithms puzzle
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/strip-packing.html">
     Extra material: Strip packing
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/job-shop-scheduling.html">
     Extra material: Job shop scheduling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../03/maintenance-planning.html">
     Extra material: Maintenance planning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../04/04.00.html">
   4. Network Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/dinner-seat-allocation.html">
     Dinner seating arrangement
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/gasoline-distribution.html">
     Gasoline distribution
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/cryptocurrency-arbitrage.html">
     Cryptocurrency arbitrage search
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/power-network.html">
     Extra material: Energy dispatch problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../04/forex-arbitrage.html">
     Forex Arbitrage
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="05.00.html">
   5. Convex Optimization
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="milk-pooling.html">
     Milk pooling and blending
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="ols-regression.html">
     Ordinary Least Squares (OLS) Regression
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="markowitz_portfolio.html">
     Markowitz portfolio optimization
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Support Vector Machines for Binary Classification
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="refinery-production.html">
     Extra material: Refinery production and shadow pricing with CVXPY
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="cutting-stock.html">
     Extra Material: Cutting Stock
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../06/06.00.html">
   6. Conic Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/economic-order-quantity.html">
     Economic Order Quantity
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/kelly-criterion.html">
     The Kelly Criterion
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/markowitz_portfolio_revisited.html">
     Markowitz portfolio optimization revisited
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/building-insulation.html">
     Optimal Design of Multilayered Building Insulation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/svm-conic.html">
     Training Support Vector Machines with Conic Optimization
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/investment-wheel.html">
     Extra material: Luenberger’s Investment Wheel
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../06/optimal-growth-portfolios.html">
     Extra material: Optimal Growth Portfolios with Risk Aversion
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../07/07.00.html">
   7. Accounting for Uncertainty: Optimization Meets Reality
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/fleet-assignment.html">
     Fleet assignment problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../07/bim-robustness-analysis.html">
     Robustness analysis of BIM production plan via simulations
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../08/08.00.html">
   8. Robust Optimization - Single Stage Problems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../08/bim-robust-optimization.html">
     Robust BIM microchip production problem
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../09/09.00.html">
   9. Stochastic Optimization - Single Stage Problems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/pop-up_shop.html">
     Pop-up shop
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/markowitz_portfolio_with_chance_constraint.html">
     Markowitz portfolio optimization with chance constraints
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/seafood.html">
     Stock optimization for seafood distribution center
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../09/economicdispatch.html">
     Economic dispatch in energy systems
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../10/10.00.html">
   10. Two-Stage Problems
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/airline-seating.html">
     Airline seat allocation problem
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/opf-ldr.html">
     Optimal power flow problem with recourse actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/ccg.html">
     Two-stage Production Planning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/farmer.html">
     Extra: The farmer’s problem and its variants
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../10/opf-wind-curtailment.html">
     Extra: Two-stage energy dispatch optimization with wind curtailment
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../appendix/appendix.html">
   Appendix: Working with Pyomo
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../appendix/pyomo-style-guide.html">
     Pyomo style guide
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../appendix/functional-programming-pyomo.html">
     Functional Programming with Pyomo
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../genindex.html">
   Index
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/mobook/MO-book/main?urlpath=tree/notebooks/05/svm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/mobook/MO-book/blob/main/notebooks/05/svm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/mobook/MO-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/mobook/MO-book/issues/new?title=Issue%20on%20page%20%2Fnotebooks/05/svm.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/notebooks/05/svm.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binary-classification">
   Binary classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-data-set">
   The data set
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#read-data">
     Read data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#select-features-and-training-sets">
     Select features and training sets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#support-vector-machines-svm">
   Support vector machines (SVM)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-svm-classifier">
     Linear SVM classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-metrics">
     Performance metrics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-optimization-model">
   Linear optimization model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pyomo-implementation">
     Pyomo implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quadratic-programming-model">
   Quadratic programming model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#primal-form">
     Primal form
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dual-formulation">
     Dual Formulation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernelized-svm">
   Kernelized SVM
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nonlinear-feature-spaces">
     Nonlinear feature spaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-kernel-trick">
     The kernel trick
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-kernel">
     Linear kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polynomial-kernels">
     Polynomial kernels
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Support Vector Machines for Binary Classification</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#binary-classification">
   Binary classification
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-data-set">
   The data set
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#read-data">
     Read data
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#select-features-and-training-sets">
     Select features and training sets
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#support-vector-machines-svm">
   Support vector machines (SVM)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-svm-classifier">
     Linear SVM classifier
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#performance-metrics">
     Performance metrics
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#linear-optimization-model">
   Linear optimization model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pyomo-implementation">
     Pyomo implementation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#quadratic-programming-model">
   Quadratic programming model
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#primal-form">
     Primal form
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#dual-formulation">
     Dual Formulation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#kernelized-svm">
   Kernelized SVM
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#nonlinear-feature-spaces">
     Nonlinear feature spaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-kernel-trick">
     The kernel trick
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#implementation">
     Implementation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#linear-kernel">
     Linear kernel
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#polynomial-kernels">
     Polynomial kernels
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <span class="target" id="index-0"></span><span class="target" id="index-1"></span><span class="target" id="index-2"></span><span class="target" id="index-3"></span><section class="tex2jax_ignore mathjax_ignore" id="support-vector-machines-for-binary-classification">
<span id="index-4"></span><h1>Support Vector Machines for Binary Classification<a class="headerlink" href="#support-vector-machines-for-binary-classification" title="Permalink to this headline">#</a></h1>
<p>Support Vector Machines (SVM) are a type of supervised machine learning model. Similar to other machine learning techniques based on regression, training an SVM classifier uses examples with known outcomes, and involves optimization some measure of performance. The resulting classifier can then be applied to classify data with unknown outcomes.</p>
<p>In this notebook, we will demonstrate the process of training an SVM for binary classification using linear and quadratic programming. Our implementation will initially focus on linear support vector machines which separate the feature space by means of a hyperplane. We will explore both primal and dual formulations. Then, using kernels, the dual formulation is extended to binary classification in higher-order and nonlinear feature spaces. Several different formulations of the optimization problem are given in Pyomo and applied to a banknote classification application.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># install Pyomo and solvers</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">SOLVER_LO</span> <span class="o">=</span> <span class="s2">&quot;clp&quot;</span>
<span class="n">SOLVER_NLO</span> <span class="o">=</span> <span class="s2">&quot;ipopt&quot;</span>

<span class="k">if</span> <span class="s1">&#39;google.colab&#39;</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">modules</span><span class="p">:</span>
    <span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>idaes-pse<span class="w"> </span>--pre<span class="w"> </span>&gt;/dev/null<span class="w"> </span><span class="m">2</span>&gt;/dev/null
    <span class="o">!</span>idaes<span class="w"> </span>get-extensions<span class="w"> </span>--to<span class="w"> </span>./bin<span class="w"> </span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s1">&#39;PATH&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="s1">&#39;:bin&#39;</span>
</pre></div>
</div>
</div>
</div>
<section id="binary-classification">
<h2>Binary classification<a class="headerlink" href="#binary-classification" title="Permalink to this headline">#</a></h2>
<p>Binary classifiers are functions designed to answer questions such as “does this medical test indicate disease?”, “will this specific customer enjoy that specific movie?”, “does this photo include a car?”, or “is this banknote genuine or counterfeit?” These questions are answered based on the values of “features” that may include physical measurements or other types of data collected from a representative data set with known outcomes.</p>
<p>In this notebook we consider a binary classifier that might be installed in a vending machine to detect banknotes. The goal of the device is to accurately identify and accept genuine banknotes while rejecting counterfeit ones. The classifier’s performance can be assessed using definitions in following table, where “positive” refers to an instance of a genuine banknote.</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p></p></th>
<th class="text-align:center head"><p>Predicted Positive</p></th>
<th class="text-align:center head"><p>Predicted Negative</p></th>
<th class="text-align:left head"><p></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p>Actual Positive</p></td>
<td class="text-align:center"><p>True Positive (TP)</p></td>
<td class="text-align:center"><p>False Negative (FN)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p>Actual Negative</p></td>
<td class="text-align:center"><p>False Positive (FP)</p></td>
<td class="text-align:center"><p>True Negative (TN)</p></td>
<td class="text-align:left"><p></p></td>
</tr>
</tbody>
</table>
<p>A vending machine user would be frustrated if a genuine banknote is incorrectly rejected as a false negative. <strong>Sensitivity</strong> is defined as the number of true positives (TP) divided by the total number of actual positives (TP + FN). A user of the vending machine would prefer high sensitivity because that means genuine banknotes are likely to be accepted.</p>
<p>The vending machine owner/operator, on the other hand, wants to avoid accepting counterfeit banknotes and would therefore prefer a low number of false positives (FP). <strong>Precision</strong> is the number of true positives (TP) divided by the total number of predicted positives (TP + FP). The owner/operate would prefer high precision because that means almost all of the accepted notes are genuine.</p>
<ul class="simple">
<li><p><strong>Sensitivity</strong>: The number of true positives divided by the total number of actual positives. High sensitivity indicates a low false negative rate.</p></li>
<li><p><strong>Precision</strong>: The number of true positives identified by the model divided by the total number of predicted positives, which includes both true and false positives. High precision indicates a low false positive rate.</p></li>
</ul>
<p>To achieve high sensitivity, a classifier can follow the “innocent until proven guilty” standard, rejecting banknotes only when certain they are counterfeit. To achieve high precision, a classifier can adopt the “guilty unless proven innocent” standard, rejecting banknotes unless absolutely certain they are genuine.</p>
<p>The challenge in developing binary classifiers is to balance these conflicting objectives and to optimize performance from both perspectives at the same time.</p>
</section>
<section id="the-data-set">
<h2>The data set<a class="headerlink" href="#the-data-set" title="Permalink to this headline">#</a></h2>
<p>The following data set contains measurements from a collection of known genuine and known counterfeit banknote specimens. The data includes four continuous statistical measures obtained from the wavelet transform of banknote images named “variance”, “skewness”, “curtosis”, and “entropy”, and a binary variable named “class” which is 0 if genuine and 1 if counterfeit.</p>
<p><a class="reference external" href="https://archive.ics.uci.edu/ml/datasets/banknote+authentication">https://archive.ics.uci.edu/ml/datasets/banknote+authentication</a></p>
<section id="read-data">
<h3>Read data<a class="headerlink" href="#read-data" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># read data set</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span>
    <span class="s2">&quot;https://raw.githubusercontent.com/mobook/MO-book/main/datasets/data_banknote_authentication.txt&quot;</span><span class="p">,</span>
    <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;skewness&quot;</span><span class="p">,</span> <span class="s2">&quot;curtosis&quot;</span><span class="p">,</span> <span class="s2">&quot;entropy&quot;</span><span class="p">,</span> <span class="s2">&quot;class&quot;</span><span class="p">]</span>
<span class="n">df</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;Banknotes&quot;</span>

<span class="c1"># show a few rows</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variance</th>
      <th>skewness</th>
      <th>curtosis</th>
      <th>entropy</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>3.62160</td>
      <td>8.6661</td>
      <td>-2.8073</td>
      <td>-0.44699</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4.54590</td>
      <td>8.1674</td>
      <td>-2.4586</td>
      <td>-1.46210</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3.86600</td>
      <td>-2.6383</td>
      <td>1.9242</td>
      <td>0.10645</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3.45660</td>
      <td>9.5228</td>
      <td>-4.0112</td>
      <td>-3.59440</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.32924</td>
      <td>-4.4552</td>
      <td>4.5718</td>
      <td>-0.98880</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># get a statistical description of the data set</span>
<span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>variance</th>
      <th>skewness</th>
      <th>curtosis</th>
      <th>entropy</th>
      <th>class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>1372.000000</td>
      <td>1372.000000</td>
      <td>1372.000000</td>
      <td>1372.000000</td>
      <td>1372.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>0.433735</td>
      <td>1.922353</td>
      <td>1.397627</td>
      <td>-1.191657</td>
      <td>0.444606</td>
    </tr>
    <tr>
      <th>std</th>
      <td>2.842763</td>
      <td>5.869047</td>
      <td>4.310030</td>
      <td>2.101013</td>
      <td>0.497103</td>
    </tr>
    <tr>
      <th>min</th>
      <td>-7.042100</td>
      <td>-13.773100</td>
      <td>-5.286100</td>
      <td>-8.548200</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>-1.773000</td>
      <td>-1.708200</td>
      <td>-1.574975</td>
      <td>-2.413450</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>0.496180</td>
      <td>2.319650</td>
      <td>0.616630</td>
      <td>-0.586650</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>2.821475</td>
      <td>6.814625</td>
      <td>3.179250</td>
      <td>0.394810</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>6.824800</td>
      <td>12.951600</td>
      <td>17.927400</td>
      <td>2.449500</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
</section>
<section id="select-features-and-training-sets">
<h3>Select features and training sets<a class="headerlink" href="#select-features-and-training-sets" title="Permalink to this headline">#</a></h3>
<p>We divide the data set into a <strong>training set</strong> for training the classifier, and a <strong>testing set</strong> for evaluating the performance of the trained classifier. In addition, we select a two dimensional subset of the features so that the results can be plotted for better exposition. Since our definition of a positive outcome corresponds to detecting a genuine banknote, the “class” feature is scaled to have values of 1 for genuine banknotes and -1 for counterfeit banknotes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># create training and validation test sets</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

<span class="c1"># select training features</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;variance&quot;</span><span class="p">,</span> <span class="s2">&quot;skewness&quot;</span><span class="p">]</span>

<span class="c1"># separate into features and outputs</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">df_train</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y_train</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">df_train</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>

<span class="c1"># separate into features and outputs</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">df_test</span><span class="p">[</span><span class="n">features</span><span class="p">]</span>
<span class="n">y_test</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">df_test</span><span class="p">[</span><span class="s2">&quot;class&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>The following cell defines a function <code class="docutils literal notranslate"><span class="pre">scatter</span></code> that produces a 2D scatter plots of a labeled features. The function assigns default labels and colors, and otherwise passes along other keyword arguments.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scatter_labeled_data</span><span class="p">(</span>
    <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;+1&quot;</span><span class="p">,</span> <span class="s2">&quot;-1&quot;</span><span class="p">],</span> <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">],</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a scatter plot for labeled data with default labels and colors.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X : DataFrame</span>
<span class="sd">        Feature matrix as a DataFrame.</span>
<span class="sd">    y : Series</span>
<span class="sd">        Target vector as a Series.</span>
<span class="sd">    labels : list, optional</span>
<span class="sd">        Labels for the positive and negative classes. Default is [&quot;+1&quot;, &quot;-1&quot;].</span>
<span class="sd">    colors : list, optional</span>
<span class="sd">        Colors for the positive and negative classes. Default is [&quot;g&quot;, &quot;r&quot;].</span>
<span class="sd">    **kwargs : dict</span>
<span class="sd">        Additional keyword arguments for the scatter plot.</span>

<span class="sd">    Returns:</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Prepend keyword arguments for all scatter plots</span>
    <span class="n">kw</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;x&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;kind&quot;</span><span class="p">:</span> <span class="s2">&quot;scatter&quot;</span><span class="p">,</span> <span class="s2">&quot;alpha&quot;</span><span class="p">:</span> <span class="mf">0.4</span><span class="p">}</span>
    <span class="n">kw</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="c1"># Ignore warnings from matplotlib scatter plot</span>
    <span class="kn">import</span> <span class="nn">warnings</span>

    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">filterwarnings</span><span class="p">(</span><span class="s2">&quot;ignore&quot;</span><span class="p">)</span>
        <span class="n">kw</span><span class="p">[</span><span class="s2">&quot;ax&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="o">**</span><span class="n">kw</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">labels</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># plot training and test sets in two axes</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
<span class="n">scatter_labeled_data</span><span class="p">(</span>
    <span class="n">X_train</span><span class="p">,</span>
    <span class="n">y_train</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;genuine&quot;</span><span class="p">,</span> <span class="s2">&quot;counterfeit&quot;</span><span class="p">],</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Training Set&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">scatter_labeled_data</span><span class="p">(</span>
    <span class="n">X_test</span><span class="p">,</span>
    <span class="n">y_test</span><span class="p">,</span>
    <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;genuine&quot;</span><span class="p">,</span> <span class="s2">&quot;counterfeit&quot;</span><span class="p">],</span>
    <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Test Set&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/svm_11_0.png" src="../../_images/svm_11_0.png" />
</div>
</div>
</section>
</section>
<section id="support-vector-machines-svm">
<h2>Support vector machines (SVM)<a class="headerlink" href="#support-vector-machines-svm" title="Permalink to this headline">#</a></h2>
<section id="linear-svm-classifier">
<h3>Linear SVM classifier<a class="headerlink" href="#linear-svm-classifier" title="Permalink to this headline">#</a></h3>
<p>A linear support vector machine (SVM) is a binary classification method that employs a linear equation to determine class assignment. The basic  formula is expressed as:</p>
<div class="math notranslate nohighlight">
\[y^{pred} = \text{sgn}\ ( w^\top x + b)\]</div>
<p>where <span class="math notranslate nohighlight">\(x\)</span> is a point <span class="math notranslate nohighlight">\(x\in\mathbb{R}^p\)</span> in “feature” space. Here <span class="math notranslate nohighlight">\(w\in \mathbb{R}^p\)</span> represents a set of coefficients, <span class="math notranslate nohighlight">\(w^\top x\)</span> is the dot product, and <span class="math notranslate nohighlight">\(b\)</span> is a scalar coefficient. The hyperplane defined by <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> separates the feature space into two classes. Points on one side of the hyperplane are have a positive outcome (+1); while points on the other side have a negative outcome (-1).</p>
<p>The following cell presents a simple Python implementation of a linear SVM. An instance of <code class="docutils literal notranslate"><span class="pre">LinearSVM</span></code> is defined with a coefficient vector <span class="math notranslate nohighlight">\(w\)</span> and a scalar <span class="math notranslate nohighlight">\(b\)</span>. In this implementation, all data and parameters are provided as Pandas Series or DataFrame objects, and the Pandas <code class="docutils literal notranslate"><span class="pre">.dot()</span></code> function is used to compute the dot product.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># import required libraries</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="c1"># Linear Support Vector Machine (SVM) class</span>
<span class="k">class</span> <span class="nc">LinearSVM</span><span class="p">:</span>
    <span class="c1"># Initialize the Linear SVM with weights and bias</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            w (Pandas Series or dictionary): Weights of the SVM</span>
<span class="sd">            b (float): Bias of the SVM</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># Call method to compute the decision function</span>
    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            X (pandas.DataFrame): Input data</span>

<span class="sd">        Returns:</span>
<span class="sd">            numpy.array: Array of decision function values</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>

    <span class="c1"># Representation method for the Linear SVM class</span>
    <span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Returns:</span>
<span class="sd">            str: String representation of the Linear SVM</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;LinearSvm(w = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">w</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span><span class="si">}</span><span class="s2">, b = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="si">}</span><span class="s2">)&quot;</span>
</pre></div>
</div>
</div>
</div>
<p>A visual inspection of the banknote training set shows the two dimensional feature set can be approximately split along a vertical axis where “variance” is zero. Most of the positive outcomes are on the right of the axis, most of the negative outcomes on the left. Since <span class="math notranslate nohighlight">\(w\)</span> is a vector normal to this surface, we choose</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
    w &amp; = \begin{bmatrix} w_{variance} \\ w_{skewness} \end{bmatrix} = \begin{bmatrix} 1 \\ 0 \end{bmatrix},
    \qquad b = 0
\end{align}
\end{split}\]</div>
<p>The code cell below evaluates the accuracy of the linear SVM by calculating the <strong>accuracy score</strong>, which is the fraction of samples that were predicted accurately.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Visual estimaate of w and b for a linear classifier</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">({</span><span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;skewness&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">})</span>
<span class="n">b</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># create an instance of LinearSVM</span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">LinearSVM</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">svm</span><span class="p">)</span>

<span class="c1"># predictions for the training set</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">svm</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="c1"># fraction of correct predictions</span>
<span class="n">accuracy</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y_pred</span> <span class="o">==</span> <span class="n">y_test</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy = </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">accuracy</span><span class="si">:</span><span class="s2"> 0.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearSvm(w = {&#39;variance&#39;: 1, &#39;skewness&#39;: 0}, b = 0.0)
Accuracy =  83.6%
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">scatter_comparison</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates scatter plots comparing actual and predicted outcomes for both training and test sets.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X : DataFrame</span>
<span class="sd">        Feature matrix as a DataFrame.</span>
<span class="sd">    y : Series</span>
<span class="sd">        Actual target vector as a Series.</span>
<span class="sd">    y_pred : Series</span>
<span class="sd">        Predicted target vector as a Series.</span>

<span class="sd">    Returns:</span>
<span class="sd">    None</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">xmin</span><span class="p">,</span> <span class="n">ymin</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
    <span class="n">xmax</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">xlim</span> <span class="o">=</span> <span class="p">[</span><span class="n">xmin</span> <span class="o">-</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="p">(</span><span class="n">xmax</span> <span class="o">-</span> <span class="n">xmin</span><span class="p">),</span> <span class="n">xmax</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="p">(</span><span class="n">xmax</span> <span class="o">-</span> <span class="n">xmin</span><span class="p">)]</span>
    <span class="n">ylim</span> <span class="o">=</span> <span class="p">[</span><span class="n">ymin</span> <span class="o">-</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="p">(</span><span class="n">ymax</span> <span class="o">-</span> <span class="n">ymin</span><span class="p">),</span> <span class="n">ymax</span> <span class="o">+</span> <span class="mf">0.05</span> <span class="o">*</span> <span class="p">(</span><span class="n">ymax</span> <span class="o">-</span> <span class="n">ymin</span><span class="p">)]</span>

    <span class="c1"># Plot training and test sets</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;genuine&quot;</span><span class="p">,</span> <span class="s2">&quot;counterfeit&quot;</span><span class="p">]</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">scatter_labeled_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;g&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">],</span> <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span> <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Actual&quot;</span>
    <span class="p">)</span>
    <span class="n">scatter_labeled_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y_pred</span><span class="p">,</span>
        <span class="n">labels</span><span class="p">,</span>
        <span class="p">[</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">],</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span>
        <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">,</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Prediction&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Plot actual positives and actual negatives</span>
    <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
    <span class="n">scatter_labeled_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">y_pred</span><span class="p">[</span><span class="n">y</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="s2">&quot;true positive&quot;</span><span class="p">,</span> <span class="s2">&quot;false negative&quot;</span><span class="p">],</span>
        <span class="p">[</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">],</span>
        <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span>
        <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Actual Positives&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">scatter_labeled_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">[</span><span class="n">y</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">y_pred</span><span class="p">[</span><span class="n">y</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">],</span>
        <span class="p">[</span><span class="s2">&quot;false positive&quot;</span><span class="p">,</span> <span class="s2">&quot;true negative&quot;</span><span class="p">],</span>
        <span class="p">[</span><span class="s2">&quot;c&quot;</span><span class="p">,</span> <span class="s2">&quot;m&quot;</span><span class="p">],</span>
        <span class="n">xlim</span><span class="o">=</span><span class="n">xlim</span><span class="p">,</span>
        <span class="n">ylim</span><span class="o">=</span><span class="n">ylim</span><span class="p">,</span>
        <span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Actual Negatives&quot;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">scatter_comparison</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/svm_18_0.png" src="../../_images/svm_18_0.png" />
<img alt="../../_images/svm_18_1.png" src="../../_images/svm_18_1.png" />
</div>
</div>
</section>
<section id="performance-metrics">
<h3>Performance metrics<a class="headerlink" href="#performance-metrics" title="Permalink to this headline">#</a></h3>
<p>The accuracy score alone is not always a reliable metric for evaluating the performance of binary classifiers. For instance, when one outcome is significantly more frequent than the other, a classifier that always predicts the more common outcome without regard to the feature vector can achieve. Moreover, in many applications, the consequences of a false positive can differ from those of a false negative. For these reasons, we seek a more comprehensive set of metrics to compare binary classifiers. A <a class="reference external" href="https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-019-6413-7">detailed discussion on this topic</a> recommends the <a class="reference external" href="https://towardsdatascience.com/the-best-classification-metric-youve-never-heard-of-the-matthews-correlation-coefficient-3bf50a2f3e9a">Matthews correlation coefficient (MCC)</a> as a reliable performance measure for binary classifiers.</p>
<p>The code below demonstrates an example of a function that evaluates the performance of a binary classifier and returns the Matthews correlation coefficient as its output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">validate</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This function calculates and displays the sensitivity, precision, and Matthews correlation coefficient</span>
<span class="sd">    (MCC) for a binary classifier based on its true labels (y_true) and predicted labels (y_pred).</span>

<span class="sd">    Args:</span>
<span class="sd">    y_true (array-like): A list or array containing the true labels of the samples.</span>
<span class="sd">    y_pred (array-like): A list or array containing the predicted labels of the samples.</span>
<span class="sd">    verbose (bool, optional): If True, the function prints and displays the calculated metrics and</span>
<span class="sd">                              confusion matrix. Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">    float: The calculated Matthews correlation coefficient (MCC).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Calculate the elements of the confusion matrix</span>
    <span class="n">true_positives</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">false_negatives</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">false_positives</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">true_negatives</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">((</span><span class="n">y_true</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">y_pred</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">))</span>
    <span class="n">total</span> <span class="o">=</span> <span class="n">true_positives</span> <span class="o">+</span> <span class="n">true_negatives</span> <span class="o">+</span> <span class="n">false_positives</span> <span class="o">+</span> <span class="n">false_negatives</span>

    <span class="c1"># Calculate the Matthews correlation coefficient (MCC)</span>
    <span class="n">mcc_numerator</span> <span class="o">=</span> <span class="p">(</span><span class="n">true_positives</span> <span class="o">*</span> <span class="n">true_negatives</span><span class="p">)</span> <span class="o">-</span> <span class="p">(</span>
        <span class="n">false_positives</span> <span class="o">*</span> <span class="n">false_negatives</span>
    <span class="p">)</span>
    <span class="n">mcc_denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
        <span class="p">(</span><span class="n">true_positives</span> <span class="o">+</span> <span class="n">false_positives</span><span class="p">)</span>
        <span class="o">*</span> <span class="p">(</span><span class="n">true_positives</span> <span class="o">+</span> <span class="n">false_negatives</span><span class="p">)</span>
        <span class="o">*</span> <span class="p">(</span><span class="n">true_negatives</span> <span class="o">+</span> <span class="n">false_positives</span><span class="p">)</span>
        <span class="o">*</span> <span class="p">(</span><span class="n">true_negatives</span> <span class="o">+</span> <span class="n">false_negatives</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">mcc</span> <span class="o">=</span> <span class="n">mcc_numerator</span> <span class="o">/</span> <span class="n">mcc_denominator</span>

    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Matthews correlation coefficient (MCC) = </span><span class="si">{</span><span class="n">mcc</span><span class="si">:</span><span class="s2">0.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="c1"># report sensitivity and precision, and accuracy</span>
        <span class="n">sensitivity</span> <span class="o">=</span> <span class="n">true_positives</span> <span class="o">/</span> <span class="p">(</span><span class="n">true_positives</span> <span class="o">+</span> <span class="n">false_negatives</span><span class="p">)</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">true_positives</span> <span class="o">/</span> <span class="p">(</span><span class="n">true_positives</span> <span class="o">+</span> <span class="n">false_positives</span><span class="p">)</span>
        <span class="n">accuracy</span> <span class="o">=</span> <span class="p">(</span><span class="n">true_positives</span> <span class="o">+</span> <span class="n">true_negatives</span><span class="p">)</span> <span class="o">/</span> <span class="n">total</span>

        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Sensitivity = </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">sensitivity</span><span class="si">:</span><span class="s2"> 0.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Precision = </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">precision</span><span class="si">:</span><span class="s2"> 0.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Accuracy = </span><span class="si">{</span><span class="mi">100</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">accuracy</span><span class="si">:</span><span class="s2"> 0.1f</span><span class="si">}</span><span class="s2">%&quot;</span><span class="p">)</span>

        <span class="c1"># Display the binary confusion matrix</span>
        <span class="n">confusion_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
            <span class="p">[</span>
                <span class="p">[</span><span class="n">true_positives</span><span class="p">,</span> <span class="n">false_negatives</span><span class="p">],</span>
                <span class="p">[</span><span class="n">false_positives</span><span class="p">,</span> <span class="n">true_negatives</span><span class="p">],</span>
            <span class="p">],</span>
            <span class="n">index</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Actual Positive&quot;</span><span class="p">,</span> <span class="s2">&quot;Actual Negative&quot;</span><span class="p">],</span>
            <span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Predicted Positive&quot;</span><span class="p">,</span> <span class="s2">&quot;Predicted Negative&quot;</span><span class="p">],</span>
        <span class="p">)</span>
        <span class="n">display</span><span class="p">(</span><span class="n">confusion_matrix</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mcc</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">svm</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">validate</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>
    <span class="n">scatter_comparison</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)</span>


<span class="c1"># train and test</span>
<span class="n">svm</span> <span class="o">=</span> <span class="n">LinearSVM</span><span class="p">({</span><span class="s2">&quot;variance&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;skewness&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">},</span> <span class="mf">0.0</span><span class="p">)</span>
<span class="n">test</span><span class="p">(</span><span class="n">svm</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearSvm(w = {&#39;variance&#39;: 1, &#39;skewness&#39;: 0}, b = 0.0) 

Matthews correlation coefficient (MCC) = 0.671
Sensitivity =  85.6%
Precision =  83.9%
Accuracy =  83.6%
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted Positive</th>
      <th>Predicted Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actual Positive</th>
      <td>125</td>
      <td>21</td>
    </tr>
    <tr>
      <th>Actual Negative</th>
      <td>24</td>
      <td>105</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/svm_21_2.png" src="../../_images/svm_21_2.png" />
<img alt="../../_images/svm_21_3.png" src="../../_images/svm_21_3.png" />
</div>
</div>
</section>
</section>
<section id="linear-optimization-model">
<h2>Linear optimization model<a class="headerlink" href="#linear-optimization-model" title="Permalink to this headline">#</a></h2>
<p>A training or validation set consists of <span class="math notranslate nohighlight">\(n\)</span> observations <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> where <span class="math notranslate nohighlight">\(y_i = \pm 1\)</span> and <span class="math notranslate nohighlight">\(x_i\in\mathbb{R}^p\)</span> for <span class="math notranslate nohighlight">\(i=1, \dots, n\)</span>. The training task is to find coefficients <span class="math notranslate nohighlight">\(w\in\mathbb{R}^p\)</span> and <span class="math notranslate nohighlight">\(b\in\mathbb{R}\)</span> to achieve high sensitivity and high precision for the validation set. All points <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> for <span class="math notranslate nohighlight">\(i\in 1, \dots, n\)</span> are successfully classified if</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
    y_i (w^\top x_i + b) &amp; &gt; 0 &amp; \forall i = 1, 2, \dots, n.
\end{align}
\]</div>
<p>As written, this condition imposes no scale for <span class="math notranslate nohighlight">\(w\)</span> or <span class="math notranslate nohighlight">\(b\)</span> (that is, if the condition is satisfied for any pair <span class="math notranslate nohighlight">\((w, b)\)</span>, then it also satisfied for <span class="math notranslate nohighlight">\((\gamma w, \gamma b)\)</span> where <span class="math notranslate nohighlight">\(\gamma &gt; 0\)</span>). To remove the ambiguity, a modified condition for correctly classified points is given by</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
y_i (w^\top x_i + b) &amp; \geq 1 &amp; \forall i = 1, 2, \dots, n
\end{align*}
\]</div>
<p>which defines a <strong>hard-margin</strong> classifier. The size of the margin is determined by the scale of <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>.</p>
<p>In practice, it is not always possible to find <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> that perfectly separate all data. The condition for a hard-margin classifier is therefore relaxed by introducing non-negative decision variables <span class="math notranslate nohighlight">\(z_i \geq 0\)</span> where</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
y_i (w^\top x_i + b) &amp; \geq 1 - z_i &amp; \forall i = 1, 2, \dots, n
\end{align*}
\]</div>
<p>The variables <span class="math notranslate nohighlight">\(z_i\)</span> measure the distance of a misclassified point from the separating hyperplane. An equivalent notation is to rearrange this expression as</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    z_i &amp; = \max(0,  1 -  y_i (w^\top x_i + b)) &amp; \forall i = 1, 2, \dots, n
\end{align*}
\]</div>
<p>which is <strong>hinge-loss</strong> function. The training problem is formulated as minimizing the hinge-loss function over all the data samples:</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
    \min_{w, b} \frac{1}{n}\sum_{i=1}^n \left(1 - y_i(w^\top x_i + b)\right)^+ .
\end{align*}
\]</div>
<p>Practice has shown that minimizing this term alone produces classifiers with large entries for <span class="math notranslate nohighlight">\(w\)</span> which performs poorly on new data samples. For that reason, <strong>regularization</strong> adds a term to penalize the magnitude of <span class="math notranslate nohighlight">\(w\)</span>. In most formulations a norm <span class="math notranslate nohighlight">\(\|w\|\)</span> is used for regularization, commonly a sum of squares such as <span class="math notranslate nohighlight">\(\|w\|_2^2\)</span>. Another choice is <span class="math notranslate nohighlight">\(\|w\|_1\)</span> which, similar to Lasso regression, may result in sparse weighting vector <span class="math notranslate nohighlight">\(w\)</span> indicating the elements of the feature vector that can be neglected for classification purposes. These considerations result in the objective function</p>
<div class="math notranslate nohighlight">
\[
    \min_{w, b}\left[ \lambda \|w\|_1 + \frac{1}{n}\sum_{i=1}^n \left(1 - y_i(w^\top x_i + b)\right)^+ \right]
\]</div>
<p>The needed weights are a solution to following LP:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min\quad  &amp; \lambda \|w\|_1 + \frac{1}{n} \sum_{i=1}^n z_i \\
\text{s.t.} \quad &amp;  z_i \geq 1 - y_i(w^\top x_i + b) &amp; \forall i = 1, \dots, n \\
&amp; z_i\geq 0 &amp; \forall i = 1, \dots, n \\
&amp; w\in\mathbb{R}^p \\
&amp; b\in\mathbb{R} \\
\end{align*}
\end{split}\]</div>
<p>This is the primal optimization problem in decision variables <span class="math notranslate nohighlight">\(w\in\mathbb{R}^p\)</span>, <span class="math notranslate nohighlight">\(b\in\mathbb{R}\)</span>, and <span class="math notranslate nohighlight">\(z\in\mathbb{R}^n\)</span>, a total of <span class="math notranslate nohighlight">\(n + p + 1\)</span> unknowns with <span class="math notranslate nohighlight">\(2n\)</span> constraints. This can be recast as a linear program with the usual technique of setting <span class="math notranslate nohighlight">\(w = w^+ - w^-\)</span> where <span class="math notranslate nohighlight">\(w^+\)</span> and <span class="math notranslate nohighlight">\(w^-\)</span> are non-negative. Then</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min\quad  &amp;\lambda \sum_{j=1}^p (w^+_j + w^-_j) + \frac{1}{n}  \sum_{i=1}^n z_i \\
\text{s.t.} \quad &amp;  z_i \geq 1 - y_i((w^+ - w^-)^\top x_i + b) &amp; \forall i = 1, \dots, n \\
&amp; z_i \geq 0 &amp; \forall i = 1, \dots, n \\
&amp; w^+_j, w^-_j \geq 0 &amp; \forall j = 1, \dots, p \\
&amp; b\in\mathbb{R} \\
\end{align*}
\end{split}\]</div>
<section id="pyomo-implementation">
<h3>Pyomo implementation<a class="headerlink" href="#pyomo-implementation" title="Permalink to this headline">#</a></h3>
<p>The Pyomo implementation is a <strong>factory</strong> function. The function accepts a set of training data, creates and solves a Pyomo ConcreteModel for <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span>, then returns a trained <code class="docutils literal notranslate"><span class="pre">LinearSVM</span></code> object that can be applied to a other feature data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyomo.environ</span> <span class="k">as</span> <span class="nn">pyo</span>


<span class="k">def</span> <span class="nf">svm_factory_lp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">lambd</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a linear support vector machine (SVM) model using linear programming.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X : DataFrame</span>
<span class="sd">        Feature matrix as a DataFrame.</span>
<span class="sd">    y : Series</span>
<span class="sd">        Target vector as a Series.</span>
<span class="sd">    lambd : float, optional</span>
<span class="sd">        Regularization parameter. Default is 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">    LinearSvm :</span>
<span class="sd">        A trained linear SVM model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">ConcreteModel</span><span class="p">()</span>

    <span class="c1"># Use dataframe columns and index to index variables and constraints</span>
    <span class="n">m</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

    <span class="c1"># Decision variables</span>
    <span class="n">m</span><span class="o">.</span><span class="n">wp</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">NonNegativeReals</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">wn</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">NonNegativeReals</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">NonNegativeReals</span><span class="p">)</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Expression</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">w</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">wp</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">-</span> <span class="n">m</span><span class="o">.</span><span class="n">wn</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Objective</span><span class="p">(</span><span class="n">sense</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">lasso</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span> <span class="o">+</span> <span class="n">lambd</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">m</span><span class="o">.</span><span class="n">wp</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">+</span> <span class="n">m</span><span class="o">.</span><span class="n">wn</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span>
        <span class="p">)</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Constraint</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">hingeloss</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span>
            <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span> <span class="o">+</span> <span class="n">m</span><span class="o">.</span><span class="n">b</span>
        <span class="p">)</span>

    <span class="n">pyo</span><span class="o">.</span><span class="n">SolverFactory</span><span class="p">(</span><span class="n">SOLVER_LO</span><span class="p">)</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">p</span><span class="p">]()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">b</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">LinearSVM</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>


<span class="c1"># Train and test</span>
<span class="n">svm_lp</span> <span class="o">=</span> <span class="n">svm_factory_lp</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test</span><span class="p">(</span><span class="n">svm_lp</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearSvm(w = {&#39;variance&#39;: 0.2425752024440025, &#39;skewness&#39;: 0.05378808957825215}, b = 0.02415619309398791) 

Matthews correlation coefficient (MCC) = 0.723
Sensitivity =  94.5%
Precision =  81.7%
Accuracy =  85.8%
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted Positive</th>
      <th>Predicted Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actual Positive</th>
      <td>138</td>
      <td>8</td>
    </tr>
    <tr>
      <th>Actual Negative</th>
      <td>31</td>
      <td>98</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/svm_24_2.png" src="../../_images/svm_24_2.png" />
<img alt="../../_images/svm_24_3.png" src="../../_images/svm_24_3.png" />
</div>
</div>
</section>
</section>
<section id="quadratic-programming-model">
<h2>Quadratic programming model<a class="headerlink" href="#quadratic-programming-model" title="Permalink to this headline">#</a></h2>
<section id="primal-form">
<h3>Primal form<a class="headerlink" href="#primal-form" title="Permalink to this headline">#</a></h3>
<p>The standard formulation of a linear support vector machine uses training sets with <span class="math notranslate nohighlight">\(p\)</span>-element feature vectors <span class="math notranslate nohighlight">\(x_i\in\mathbb{R}^p\)</span> along with classification labels for those vectors, <span class="math notranslate nohighlight">\(y_i = \pm 1\)</span>. A classifier is defined by two parameters: a weight vector <span class="math notranslate nohighlight">\(w\in\mathbb{R}^p\)</span> and a bias term <span class="math notranslate nohighlight">\(b\in\mathbb{R}\)</span></p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
y^{pred} &amp; = \text{sgn}(w^\top x + b)
\end{align*}
\]</div>
<p>If a separating hyperplane exists, then we choose <span class="math notranslate nohighlight">\(w\)</span> and <span class="math notranslate nohighlight">\(b\)</span> so that a hard-margin classifier exists for the training set <span class="math notranslate nohighlight">\((x_i, y_i)\)</span> where</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
y_i \left( w^\top x_i + b \right) &amp; \geq 1 &amp; \forall i \in 1, 2, \dots, n
\end{align*}
\]</div>
<p>This can always be done if a separating hyperplane exists. But if a separating hyperplane does not exist, we introduce non-negative slack variables <span class="math notranslate nohighlight">\(z_i\)</span> to relax the constraints and settle for a soft-margin classifier</p>
<div class="math notranslate nohighlight">
\[
\begin{align*}
y_i \left( w^\top x_i + b \right) &amp; \geq 1 - z_i&amp; \forall i \in 1, 2, \dots, n
\end{align*}
\]</div>
<p>The training objective is to minimize the total distance to misclassified data points. This leads to the optimization problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min\quad  &amp; \frac{1}{2} \|w \|_2^2 + \frac{c}{n} \sum_{i=1}^n z_i \\
\text{s.t.} \quad &amp;  z_i \geq 1 - y_i(w^\top x_i + b) &amp; \forall i = 1, \dots, n \\
&amp; z_i\geq 0 &amp; \forall i = 1, \dots, n \\
&amp; w\in\mathbb{R}^p \\
&amp; b\in\mathbb{R} \\
\end{align*}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\frac{1}{2} \|\bar{w}\|_2^2\)</span> is included to regularize the solution for <span class="math notranslate nohighlight">\(w\)</span>. Choosing larger values of <span class="math notranslate nohighlight">\(c\)</span> will reduce the number and size of misclassifications. The trade-off will be larger weights <span class="math notranslate nohighlight">\(w\)</span> and the accompanying risk of over over-fitting the training data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyomo.environ</span> <span class="k">as</span> <span class="nn">pyo</span>


<span class="k">def</span> <span class="nf">svm_factory_qp</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a linear support vector machine (SVM) model using quadratic programming.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X : DataFrame</span>
<span class="sd">        Feature matrix as a DataFrame.</span>
<span class="sd">    y : Series</span>
<span class="sd">        Target vector as a Series.</span>
<span class="sd">    c : float, optional</span>
<span class="sd">        Regularization parameter. Default is 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">    LinearSvm :</span>
<span class="sd">        A trained linear SVM model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">ConcreteModel</span><span class="p">()</span>

    <span class="c1"># Use dataframe columns and index to index variables and constraints</span>
    <span class="n">m</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

    <span class="c1"># Decision variables</span>
    <span class="n">m</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">z</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="n">domain</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">NonNegativeReals</span><span class="p">)</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Objective</span><span class="p">(</span><span class="n">sense</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">qp</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">c</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">))</span> <span class="o">*</span> <span class="nb">sum</span><span class="p">(</span>
            <span class="n">m</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span>
        <span class="p">)</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Constraint</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">hingeloss</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">i</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">z</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span>
            <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">*</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span> <span class="o">+</span> <span class="n">m</span><span class="o">.</span><span class="n">b</span>
        <span class="p">)</span>

    <span class="n">pyo</span><span class="o">.</span><span class="n">SolverFactory</span><span class="p">(</span><span class="n">SOLVER_NLO</span><span class="p">)</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="n">w</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">p</span><span class="p">]()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">m</span><span class="o">.</span><span class="n">b</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">LinearSVM</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>


<span class="c1"># Train and test</span>
<span class="n">svm_qp</span> <span class="o">=</span> <span class="n">svm_factory_qp</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test</span><span class="p">(</span><span class="n">svm_qp</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearSvm(w = {&#39;variance&#39;: 0.360076173842816, &#39;skewness&#39;: 0.1186747617541338}, b = -0.10042149128145099) 

Matthews correlation coefficient (MCC) = 0.732
Sensitivity =  91.8%
Precision =  84.3%
Accuracy =  86.5%
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted Positive</th>
      <th>Predicted Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actual Positive</th>
      <td>134</td>
      <td>12</td>
    </tr>
    <tr>
      <th>Actual Negative</th>
      <td>25</td>
      <td>104</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/svm_26_2.png" src="../../_images/svm_26_2.png" />
<img alt="../../_images/svm_26_3.png" src="../../_images/svm_26_3.png" />
</div>
</div>
</section>
<section id="dual-formulation">
<h3>Dual Formulation<a class="headerlink" href="#dual-formulation" title="Permalink to this headline">#</a></h3>
<p>The dual formulation for the SVM provides insight into how a linear SVM works and essential for extending SVM to nonlinear classification. The dual formulation begins by creating a differentiable Lagrangian with dual variables <span class="math notranslate nohighlight">\(\alpha_i \geq 0\)</span> and <span class="math notranslate nohighlight">\(\beta_i \geq 0\)</span> for <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>. The task is to find saddle points of</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L} &amp; = \frac{1}{2} \|w\|_2^2 + \frac{c}{n}\sum_{i=1}^n z_i + \sum_{i=1}^n \alpha_i \left(1 - z_i - y_i (w^\top x_i + b) \right) + \sum_{i=1}^n \beta_i (-z_i) \\
\end{align*}
\end{split}\]</div>
<p>Taking derivatives with respect to the primal variables</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\frac{\partial \mathcal{L}}{\partial z_i} &amp; = \frac{c}{n} - \alpha_i - \beta_i = 0 \implies 0 \leq \alpha_i \leq \frac{c}{n}\\
\frac{\partial \mathcal{L}}{\partial w} &amp; = w  - \sum_{i=1}^n \alpha_i y_i x_i = 0 \implies  w = \sum_{i=1}^n \alpha_i y_i x_i \\
\frac{\partial \mathcal{L}}{\partial b} &amp; = - \sum_{i=1}^n \alpha_i y_i = 0 \implies \sum_{i=1}^n \alpha_i y_i = 0
\end{align*}
\end{split}\]</div>
<p>This can be arranged in the form of a standard quadratic program in <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(\alpha_i\)</span> for <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{\alpha_i}\ &amp; \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j ( x_i^\top x_j ) -  \sum_{i=1}^n \alpha_i \\
\text{s. t.}\quad &amp; \sum_{i=1}^n \alpha_i y_i = 0  \\
&amp; \alpha_i \in \left[0, \frac{c}{n}\right] &amp; i = 1, \dots, n \\
\end{align*}
\end{split}\]</div>
<p>The symmetric <span class="math notranslate nohighlight">\(n \times n\)</span> <strong>Gram matrix</strong> is defined as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    G = \begin{bmatrix} 
        (y_1 x_1^\top)(y_1 x_1) &amp; \dots &amp; (y_1 x_1^\top)(y_n x_n) \\ 
        \vdots &amp; \ddots &amp; \vdots \\ 
        (y_n x_n^\top)(y_1 x_1) &amp; \dots &amp; (y_n x_n^\top)(y_n x_n)
    \end{bmatrix}
\end{split}\]</div>
<p>where each entry is dot product of two vectors <span class="math notranslate nohighlight">\((y_i x_i), (y_j x_j) \in \mathbb{R}^{p+1}\)</span>.</p>
<p>Compared to the primal, the dual formulation appears to have reduced the number of decision variables from <span class="math notranslate nohighlight">\(n + p + 1\)</span> to <span class="math notranslate nohighlight">\(n\)</span>. But this has come with the penalty of introducing a dense matrix with <span class="math notranslate nohighlight">\(n^2\)</span> coefficients and potential processing time of order <span class="math notranslate nohighlight">\(n^3\)</span>. For large training sets where <span class="math notranslate nohighlight">\(n\sim 10^4-10^6\)</span> or even larger, this becomes a prohibitively expensive calculation. In addition, the Gram matrix will be rank deficient for cases <span class="math notranslate nohighlight">\(p&lt; n\)</span>.</p>
<p>We can eliminates the need to compute and store the full Gram matrix <span class="math notranslate nohighlight">\(G\)</span> by introducing the <span class="math notranslate nohighlight">\(n \times p\)</span> matrix <span class="math notranslate nohighlight">\(F\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    F = \begin{bmatrix} y_1 x_1^\top \\ y_2 x_2^\top \\ \vdots \\ y_n x_n^\top \end{bmatrix}
\end{split}\]</div>
<p>Then <span class="math notranslate nohighlight">\(G = FF^\top\)</span> which brings the <span class="math notranslate nohighlight">\(p\)</span> primal variables <span class="math notranslate nohighlight">\(w = F^\top\alpha\)</span> back into the computational problem. The optimization problem becomes</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{\alpha_i}\ &amp; \frac{1}{2} w^\top w -  1^\top\alpha \\
\text{s. t.}\quad &amp; y^\top\alpha = 0 \\
&amp; w = F^\top\alpha &amp; w\in\mathbb{R}^p \\
&amp; 0 \leq \alpha_i \leq \frac{c}{n} &amp; \alpha\in\mathbb{R}^n \\
\end{align*}
\end{split}\]</div>
<p>The solution for the bias term <span class="math notranslate nohighlight">\(b\)</span> is obtained by considering the complementarity conditions on the dual variables. The slack variables <span class="math notranslate nohighlight">\(z_i\)</span> are zero if <span class="math notranslate nohighlight">\(\beta_i &gt; 0\)</span> which is equivalent to <span class="math notranslate nohighlight">\(\alpha_i &lt; \frac{c}{n}\)</span>. If <span class="math notranslate nohighlight">\(\alpha_i &gt; 0\)</span> then <span class="math notranslate nohighlight">\(1 - y_i (w^\top x_i + b)\)</span>. Putting these facts together gives a formula for <span class="math notranslate nohighlight">\(b\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align}
b &amp; = y_i - w^\top x_i &amp; \forall i\in 1, 2, \ldots, n\quad \text{s.t.}\quad 0 &lt; \alpha_i &lt; \frac{c}{n}\\
\end{align}
\end{split}\]</div>
<p>This model is implemented below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pyomo.environ</span> <span class="k">as</span> <span class="nn">pyo</span>


<span class="k">def</span> <span class="nf">svm_factory_dual</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a linear support vector machine (SVM) model using the dual formulation</span>
<span class="sd">    and quadratic programming.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X : DataFrame</span>
<span class="sd">        Feature matrix as a DataFrame.</span>
<span class="sd">    y : Series</span>
<span class="sd">        Target vector as a Series.</span>
<span class="sd">    c : float, optional</span>
<span class="sd">        Regularization parameter. Default is 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">    LinearSvm :</span>
<span class="sd">        A trained linear SVM model.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">m</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">ConcreteModel</span><span class="p">()</span>

    <span class="c1"># Use dataframe columns and index to index variables and constraints</span>
    <span class="n">m</span><span class="o">.</span><span class="n">P</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

    <span class="c1"># Model parameters</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">c</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Decision variables</span>
    <span class="n">m</span><span class="o">.</span><span class="n">w</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Objective</span><span class="p">(</span><span class="n">sense</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">qp</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Constraint</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Constraint</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">projection</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">p</span><span class="p">]</span> <span class="o">==</span> <span class="nb">sum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">p</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Solve QP with the interior point method</span>
    <span class="n">pyo</span><span class="o">.</span><span class="n">SolverFactory</span><span class="p">(</span><span class="n">SOLVER_NLO</span><span class="p">)</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="c1"># Extract solution</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">w</span><span class="p">[</span><span class="n">p</span><span class="p">]()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">P</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Find alpha closest to the center of [0, c/n]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">[(</span><span class="n">a</span> <span class="o">-</span> <span class="n">C</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">argmin</span><span class="p">()]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">LinearSVM</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>


<span class="c1"># Train and test</span>
<span class="n">svm_dual</span> <span class="o">=</span> <span class="n">svm_factory_dual</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test</span><span class="p">(</span><span class="n">svm_dual</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>LinearSvm(w = {&#39;variance&#39;: 0.36007198016823705, &#39;skewness&#39;: 0.1186714426688388}, b = -0.10034400378566466) 

Matthews correlation coefficient (MCC) = 0.732
Sensitivity =  91.8%
Precision =  84.3%
Accuracy =  86.5%
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted Positive</th>
      <th>Predicted Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actual Positive</th>
      <td>134</td>
      <td>12</td>
    </tr>
    <tr>
      <th>Actual Negative</th>
      <td>25</td>
      <td>104</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/svm_28_2.png" src="../../_images/svm_28_2.png" />
<img alt="../../_images/svm_28_3.png" src="../../_images/svm_28_3.png" />
</div>
</div>
</section>
</section>
<section id="kernelized-svm">
<h2>Kernelized SVM<a class="headerlink" href="#kernelized-svm" title="Permalink to this headline">#</a></h2>
<section id="nonlinear-feature-spaces">
<h3>Nonlinear feature spaces<a class="headerlink" href="#nonlinear-feature-spaces" title="Permalink to this headline">#</a></h3>
<p>A linear SVM assumes the existence of a linear hyperplane that separates labeled sets of data points. Frequently, however, this is not possible and some sort of nonlinear method is needed.</p>
<p>Consider a binary classification done given by a function</p>
<div class="math notranslate nohighlight">
\[y^{pred} = \text{sgn} \left( w^\top \phi(x) + b \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\phi(x)\)</span> is a function mapping <span class="math notranslate nohighlight">\(x\)</span> into a higher dimensional “feature space”. That is, <span class="math notranslate nohighlight">\(\phi : \mathbb{R}^{p} \rightarrow \mathbb{R}^d\)</span> where <span class="math notranslate nohighlight">\(d \geq p \)</span>. The additional dimensions may include features such as powers of the terms in <span class="math notranslate nohighlight">\(x\)</span>, or products of those terms, or other types of nonlinear transformations. As before, we wish to find a choice for <span class="math notranslate nohighlight">\(w\in\mathbb{R}^d\)</span> such that the soft-margin classifier</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
y_i \left( w^\top \phi(x_i) + b \right) &amp; \geq 1 - z_i  &amp; i = 1, 2, \ldots, n
\end{align}
\]</div>
<p>Using the machinery as before, we set up the Lagrangian</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\mathcal{L} &amp; = \frac{1}{2} \|w\|_2^2 + \frac{c}{n}\sum_{i=1}^n z_i + \sum_{i=1}^n \alpha_i \left( 1 - z_i - y_i \left( w^\top \phi(x_i) + b \right)\right) + \sum_{i=1}^n \beta_i (-z_i) \\
\end{align*}
\end{split}\]</div>
<p>then take derivatives to find</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
    \frac{\partial \mathcal{L}}{\partial z_i} &amp; = \frac{c}{n} - \alpha_i - \beta_i = 0 \implies 0 \leq \alpha_i \leq \frac{c}{n}\\
    \frac{\partial \mathcal{L}}{\partial w} &amp; = w  - \sum_{i=1}^n \alpha_i y_i \phi(x_i) = 0 \implies  w = \sum_{i=1}^n \alpha_i y_i \phi(x_i) \\
\frac{\partial \mathcal{L}}{\partial b} &amp; = - \frac{c}{n}\sum_{i=1}^n \alpha_i y_i = 0 \implies \sum_{i=1}^n \alpha_i y_i = 0
\end{align*}
\end{split}\]</div>
<p>This is similar to the case of a linear SVM, but now the vector of weights <span class="math notranslate nohighlight">\(w\in\mathbb{R}^d\)</span> which can be a  high dimensional space with nonlinear features. Working through the algebra, we are once again left with a quadratic program in <span class="math notranslate nohighlight">\(n\)</span> variables <span class="math notranslate nohighlight">\(\alpha_i\)</span> for <span class="math notranslate nohighlight">\(i = 1, \dots, n\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{\alpha_i}\quad  &amp; \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j  \phi(x_i)^\top \phi(x_j) -  \sum_{i=1}^n \alpha_i \\
\text{s.t.}\quad &amp; \alpha_i \in \left[0, \frac{c}{n}\right] &amp; i = 1, \dots, n \\
&amp; \sum_{i=1}^n \alpha_i y_i = 0 \\
\end{align*}
\end{split}\]</div>
<p>where the resulting classifier is given by</p>
<div class="math notranslate nohighlight">
\[y^{pred} = \text{sgn} \left( \sum_{i=1}^n \alpha_i y_i \phi(x_i)^\top \phi(x) + b \right)\]</div>
</section>
<section id="the-kernel-trick">
<h3>The kernel trick<a class="headerlink" href="#the-kernel-trick" title="Permalink to this headline">#</a></h3>
<p>This is an interesting situation where the separating hyperplane is embedded in a high dimensional space of nonlinear features determined by the mapping <span class="math notranslate nohighlight">\(\phi(x)\)</span>, but all we need for computation are the inner products  <span class="math notranslate nohighlight">\(\phi(x_i)^\top\phi(x_j)\)</span> to train the classifier, and the inner products <span class="math notranslate nohighlight">\(\phi(x_i)^\top\phi(x)\)</span> to use the classifier. If we had a function <span class="math notranslate nohighlight">\(K(x, z)\)</span> that returned the value <span class="math notranslate nohighlight">\(\phi(x)^\top\phi(z)\)</span> then we would never need to actually compute <span class="math notranslate nohighlight">\(\phi(x)\)</span>, <span class="math notranslate nohighlight">\(\phi(z)\)</span> or their inner product.</p>
<p>Mercer’s theorem turns the analysis on its head by specifying conditions for which a function <span class="math notranslate nohighlight">\(K(x, z)\)</span> to be expressed as an inner product for some <span class="math notranslate nohighlight">\(\phi(x)\)</span>. If <span class="math notranslate nohighlight">\(K(x, z)\)</span> is symmetric (i.e, <span class="math notranslate nohighlight">\(K(x, z) = K(z, x)\)</span>, and if the Gram matrix constructed for any collection of points <span class="math notranslate nohighlight">\(x_1, x_2, \ldots, x_n\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{bmatrix} 
    K(x_1, x_1) &amp; \dots &amp; K(x_1, x_n) \\ 
    \vdots &amp; \ddots &amp; \vdots \\ 
    K(x_n, x_1) &amp; \dots &amp; K(x_n, x_n) 
\end{bmatrix}
\end{split}\]</div>
<p>is positive semi-definite, then there is some <span class="math notranslate nohighlight">\(\phi(x)\)</span> for which <span class="math notranslate nohighlight">\(K(x, z)\)</span> is an inner product. We call such functions kernels. The practical consequence is that we can train and implement nonlinear classifiers using kernel and without ever needing to compute the higher dimensional features. This remarkable result is called the “kernel trick”.</p>
</section>
<section id="implementation">
<h3>Implementation<a class="headerlink" href="#implementation" title="Permalink to this headline">#</a></h3>
<p>To take advantage of the kernel trick, we assume an appropriate kernel <span class="math notranslate nohighlight">\(K(x, z)\)</span> has been identified, then replace all instances of <span class="math notranslate nohighlight">\(\phi(x_i)^\top \phi(x)\)</span> with the kernel. The “kernelized” SVM is given by a solution to</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min_{\alpha_i}\quad &amp; \frac{1}{2} \sum_{i=1}^n\sum_{j=1}^n \alpha_i \alpha_j y_i y_j K(x_i, x_j) -  \sum_{i=1}^n \alpha_i \\
\text{s.t.}\quad &amp; \sum_{i=1}^n \alpha_i y_i = 0 \\
&amp; \alpha_i \in \left[0, \frac{c}{n}\right] &amp; i = 1, \dots, n \\
\end{align*}
\end{split}\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[
\begin{align}
b &amp; = y_i - \sum_{j=1}^n \alpha_j y_j K(x_j, x_i) &amp; \forall i\in 1, 2, \ldots, n\quad \text{s.t.}\quad 0 &lt; \alpha_i &lt; \frac{c}{n}
\end{align}
\]</div>
<p>where the resulting classifier is given by</p>
<div class="math notranslate nohighlight">
\[y^{pred} = \text{sgn} \left( \sum_{i=1}^n \alpha_i y_i K(x_i, x) + b \right)\]</div>
<p>We define the <span class="math notranslate nohighlight">\(n\times n\)</span> positive symmetric semi-definite Gram matrix</p>
<div class="math notranslate nohighlight">
\[\begin{split}
G = \begin{bmatrix} 
    y_1 y_1 K(x_1, x_1) &amp; \dots &amp; y_1 y_n K(x_1, x_n) \\ 
    \vdots &amp; \ddots &amp; \vdots \\ 
    y_n y_1 K(x_n, x_1) &amp; \dots &amp; y_n y_n K(x_n, x_n) 
\end{bmatrix}
\end{split}\]</div>
<p>We factor <span class="math notranslate nohighlight">\(G = F F^\top\)</span> where <span class="math notranslate nohighlight">\(F\)</span> has dimensions <span class="math notranslate nohighlight">\(n \times q\)</span> and where <span class="math notranslate nohighlight">\(q\)</span> is the rank of <span class="math notranslate nohighlight">\(G\)</span>. The factorization is not unique. As demonstrated in the Python code below, one suitable factorization is the spectral factorization <span class="math notranslate nohighlight">\(G = U\Lambda U^T\)</span> where <span class="math notranslate nohighlight">\(\Lambda\)</span> is a <span class="math notranslate nohighlight">\(q\times q\)</span> diagonal matrix of non-zero eigenvalues, and <span class="math notranslate nohighlight">\(U\)</span> is an <span class="math notranslate nohighlight">\(n\times q\)</span> normal matrix such that <span class="math notranslate nohighlight">\(U^\top U = I_q\)</span>. Then</p>
<div class="math notranslate nohighlight">
\[F = U\Lambda^{1/2}\]</div>
<p>Once this factorization is complete, the optimization problem for the kernalized SVM is the same as for the linear SVM in the dual formulation</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min\quad &amp; \frac{1}{2} \alpha^\top F F^\top \alpha -  1^\top \alpha \\
\text{s.t.}\quad &amp; \sum_{i=1}^n \alpha_i y_i = 0 \\
&amp; 0 \leq \alpha_i \leq \frac{c}{n} &amp; \alpha\in\mathbb{R}^n \\
\end{align*}
\end{split}\]</div>
<p>The result is a quadratic program for the dual coefficients <span class="math notranslate nohighlight">\(\alpha\)</span> and auxiliary variables <span class="math notranslate nohighlight">\(v\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{align*}
\min\quad &amp; \frac{1}{2} v^\top v - 1^\top \alpha\\
\text{s.t.}\quad &amp; y^\top \alpha = 1 \\
&amp; v = F^\top \alpha &amp; u\in\mathbb{R}^{q} \\
&amp; 0 \leq \alpha_i \leq \frac{c}{n} &amp; \alpha\in\mathbb{R}^n \\
\end{align*}
\end{split}\]</div>
<p>Summarizing, the essential difference between training the linear and kernelized SVM is the need to compute and factor the Gram matrix. The result will be a set of non-zero coefficients <span class="math notranslate nohighlight">\(\alpha_i &gt; 0\)</span> the define a set of support vectors <span class="math notranslate nohighlight">\(\mathcal{SV}\)</span>. The classifier is then given by</p>
<div class="math notranslate nohighlight">
\[y^{pred} = \text{sgn} \left( \sum_{i\in\mathcal{SV}} \alpha_i y_i K(x_i, x) + b \right)\]</div>
<p>The implementation of the kernelized SVM is split into two parts. The first part is a class used to create instances of the classifier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>


<span class="k">class</span> <span class="nc">KernelSVM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Kernel Support Vector Machine (SVM) class.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">kernel</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Initialize the Kernel SVM with weights and bias.</span>

<span class="sd">        :param X: numpy array or list, training data.</span>
<span class="sd">        :param y: numpy array or list, target labels.</span>
<span class="sd">        :param a: numpy array or list, alpha values for the support vectors.</span>
<span class="sd">        :param b: float, bias value.</span>
<span class="sd">        :param kernel: function, kernel function to be used in the SVM.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">a</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">b</span> <span class="o">=</span> <span class="n">b</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">kernel</span> <span class="o">=</span> <span class="n">kernel</span>

    <span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Z</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compute the decision function.</span>

<span class="sd">        :param Z: pandas DataFrame, test data.</span>
<span class="sd">        :return: pandas Series, predicted labels.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">K</span> <span class="o">=</span> <span class="p">[</span>
            <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">kernel</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">Z</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">Z</span><span class="o">.</span><span class="n">index</span><span class="p">]</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">X</span><span class="p">))</span>
        <span class="p">]</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sign</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">u</span> <span class="o">@</span> <span class="n">K</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">b</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">Z</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The second part of the implementation is a factory function containing the optimization model for training an SVM. Given training data and a kernal function, the factory returns an instance of a kernelized SVM. The default is a linear kernel.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">pyomo.environ</span> <span class="k">as</span> <span class="nn">pyo</span>


<span class="k">def</span> <span class="nf">svm_factory_kernel</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="n">x</span> <span class="o">@</span> <span class="n">z</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Creates a kernel-based support vector machine (SVM) model.</span>

<span class="sd">    Parameters:</span>
<span class="sd">    X : DataFrame</span>
<span class="sd">        Feature matrix as a DataFrame.</span>
<span class="sd">    y : Series</span>
<span class="sd">        Target vector as a Series.</span>
<span class="sd">    c : float, optional</span>
<span class="sd">        Regularization parameter. Default is 1.</span>
<span class="sd">    tol : float, optional</span>
<span class="sd">        Tolerance for eigenvalue threshold. Default is 1e-8.</span>
<span class="sd">    kernel : callable, optional</span>
<span class="sd">        Kernel function that accepts two input vectors and returns a scalar. Default is the linear kernel.</span>

<span class="sd">    Returns:</span>
<span class="sd">    kernelSVM : callable</span>
<span class="sd">        A trained kernel-based SVM model as a callable function.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># Convert to numpy arrays for speed improvement</span>
    <span class="n">n</span><span class="p">,</span> <span class="n">p</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">X_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>
    <span class="n">y_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">to_numpy</span><span class="p">()</span>

    <span class="c1"># Gram matrix</span>
    <span class="n">G</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
            <span class="n">G</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">G</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">y_</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">y_</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X_</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X_</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:])</span>

    <span class="c1"># Factor the Gram matrix</span>
    <span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">G</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">eigvals</span> <span class="o">&gt;=</span> <span class="n">tol</span> <span class="o">*</span> <span class="nb">max</span><span class="p">(</span><span class="n">eigvals</span><span class="p">)</span>
    <span class="n">F</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span>
        <span class="n">eigvecs</span><span class="p">[:,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">eigvals</span><span class="p">[</span><span class="n">idx</span><span class="p">])),</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span>
    <span class="p">)</span>

    <span class="c1"># Build model</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">ConcreteModel</span><span class="p">()</span>

    <span class="c1"># Use dataframe columns and index to index variables and constraints</span>
    <span class="n">m</span><span class="o">.</span><span class="n">Q</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">N</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Set</span><span class="p">(</span><span class="n">initialize</span><span class="o">=</span><span class="n">F</span><span class="o">.</span><span class="n">index</span><span class="p">)</span>

    <span class="c1"># Model parameters</span>
    <span class="n">C</span> <span class="o">=</span> <span class="n">c</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Decision variables</span>
    <span class="n">m</span><span class="o">.</span><span class="n">u</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>
    <span class="n">m</span><span class="o">.</span><span class="n">a</span> <span class="o">=</span> <span class="n">pyo</span><span class="o">.</span><span class="n">Var</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">,</span> <span class="n">bounds</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">C</span><span class="p">))</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Objective</span><span class="p">(</span><span class="n">sense</span><span class="o">=</span><span class="n">pyo</span><span class="o">.</span><span class="n">minimize</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">qp</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">u</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Constraint</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">bias</span><span class="p">(</span><span class="n">m</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">sum</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="nd">@m</span><span class="o">.</span><span class="n">Constraint</span><span class="p">(</span><span class="n">m</span><span class="o">.</span><span class="n">Q</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">projection</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">q</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">m</span><span class="o">.</span><span class="n">u</span><span class="p">[</span><span class="n">q</span><span class="p">]</span> <span class="o">==</span> <span class="nb">sum</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">q</span><span class="p">]</span> <span class="o">*</span> <span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Solve QP with the interior point method</span>
    <span class="n">pyo</span><span class="o">.</span><span class="n">SolverFactory</span><span class="p">(</span><span class="n">SOLVER_NLO</span><span class="p">)</span><span class="o">.</span><span class="n">solve</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>

    <span class="c1"># Extract solution</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]()</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">)</span>

    <span class="c1"># Find b by locating a closest to the center of [0, c/n]</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">a</span><span class="o">.</span><span class="n">index</span><span class="p">[(</span><span class="n">a</span> <span class="o">-</span> <span class="n">C</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span><span class="o">.</span><span class="n">argmin</span><span class="p">()]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="nb">sum</span><span class="p">(</span>
        <span class="p">[</span><span class="n">a</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">*</span> <span class="n">kernel</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">j</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="c1"># Display the support vectors</span>
    <span class="n">y_support</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">(</span>
        <span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]()</span> <span class="o">&gt;</span> <span class="mf">1e-4</span> <span class="o">*</span> <span class="n">C</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span><span class="p">],</span> <span class="n">index</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">index</span>
    <span class="p">)</span>
    <span class="n">scatter_labeled_data</span><span class="p">(</span>
        <span class="n">X</span><span class="p">,</span>
        <span class="n">y_support</span><span class="p">,</span>
        <span class="n">colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;b&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span>
        <span class="n">labels</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Support Vector&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">],</span>
        <span class="n">title</span><span class="o">=</span><span class="s2">&quot;Support Vectors&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Find support vectors</span>
    <span class="n">SV</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">m</span><span class="o">.</span><span class="n">N</span> <span class="k">if</span> <span class="n">m</span><span class="o">.</span><span class="n">a</span><span class="p">[</span><span class="n">i</span><span class="p">]()</span> <span class="o">&gt;</span> <span class="mf">1e-3</span> <span class="o">*</span> <span class="n">C</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">KernelSVM</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">SV</span><span class="p">,</span> <span class="p">:],</span> <span class="n">y</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">SV</span><span class="p">],</span> <span class="n">a</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">SV</span><span class="p">],</span> <span class="n">b</span><span class="p">,</span> <span class="n">kernel</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="linear-kernel">
<h3>Linear kernel<a class="headerlink" href="#linear-kernel" title="Permalink to this headline">#</a></h3>
<p>For comparison with the previous cases, the first kernel we consider is a linear kernel</p>
<div class="math notranslate nohighlight">
\[ K(x, z) = x^\top z \]</div>
<p>which should reproduce the results obtained earlier.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">svm_kernel</span> <span class="o">=</span> <span class="n">svm_factory_kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test</span><span class="p">(</span><span class="n">svm_kernel</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;__main__.KernelSVM object at 0x7f7dc5f44e80&gt; 

Matthews correlation coefficient (MCC) = 0.732
Sensitivity =  91.8%
Precision =  84.3%
Accuracy =  86.5%
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted Positive</th>
      <th>Predicted Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actual Positive</th>
      <td>134</td>
      <td>12</td>
    </tr>
    <tr>
      <th>Actual Negative</th>
      <td>25</td>
      <td>104</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/svm_34_2.png" src="../../_images/svm_34_2.png" />
<img alt="../../_images/svm_34_3.png" src="../../_images/svm_34_3.png" />
<img alt="../../_images/svm_34_4.png" src="../../_images/svm_34_4.png" />
</div>
</div>
</section>
<section id="polynomial-kernels">
<h3>Polynomial kernels<a class="headerlink" href="#polynomial-kernels" title="Permalink to this headline">#</a></h3>
<p>A polynomial kernel of order <span class="math notranslate nohighlight">\(d\)</span> has the form</p>
<div class="math notranslate nohighlight">
\[K(x, z) = (1 + x^\top z)^d\]</div>
<p>The following cell demonstrates a quadratic kernel applied to the banknote data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">quadratic</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span> <span class="o">@</span> <span class="n">z</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

<span class="n">svm_kernel</span> <span class="o">=</span> <span class="n">svm_factory_kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">quadratic</span><span class="p">)</span>
<span class="n">test</span><span class="p">(</span><span class="n">svm_kernel</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;__main__.KernelSVM object at 0x7f7de20941c0&gt; 

Matthews correlation coefficient (MCC) = 0.825
Sensitivity =  93.2%
Precision =  90.7%
Accuracy =  91.3%
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted Positive</th>
      <th>Predicted Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actual Positive</th>
      <td>136</td>
      <td>10</td>
    </tr>
    <tr>
      <th>Actual Negative</th>
      <td>14</td>
      <td>115</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/svm_36_2.png" src="../../_images/svm_36_2.png" />
<img alt="../../_images/svm_36_3.png" src="../../_images/svm_36_3.png" />
<img alt="../../_images/svm_36_4.png" src="../../_images/svm_36_4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">cubic</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">z</span><span class="p">:</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">x</span> <span class="o">@</span> <span class="n">z</span><span class="p">)</span> <span class="o">**</span> <span class="mi">3</span>

<span class="n">svm_kernel</span> <span class="o">=</span> <span class="n">svm_factory_kernel</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">kernel</span><span class="o">=</span><span class="n">cubic</span><span class="p">)</span>
<span class="n">test</span><span class="p">(</span><span class="n">svm_kernel</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;__main__.KernelSVM object at 0x7f7de234bc70&gt; 

Matthews correlation coefficient (MCC) = 0.856
Sensitivity =  90.4%
Precision =  95.7%
Accuracy =  92.7%
</pre></div>
</div>
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Predicted Positive</th>
      <th>Predicted Negative</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Actual Positive</th>
      <td>132</td>
      <td>14</td>
    </tr>
    <tr>
      <th>Actual Negative</th>
      <td>6</td>
      <td>123</td>
    </tr>
  </tbody>
</table>
</div></div><img alt="../../_images/svm_37_2.png" src="../../_images/svm_37_2.png" />
<img alt="../../_images/svm_37_3.png" src="../../_images/svm_37_3.png" />
<img alt="../../_images/svm_37_4.png" src="../../_images/svm_37_4.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks/05"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="markowitz_portfolio.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Markowitz portfolio optimization</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="refinery-production.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Extra material: Refinery production and shadow pricing with CVXPY</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The MO Book Group<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>